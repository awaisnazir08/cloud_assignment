{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "k_jUOr_jtjKj",
        "outputId": "c6ec4b35-68f7-4857-b99f-cbfe813476b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Fgf4OJDtetR"
      },
      "source": [
        "# Fine-tuning Stable Diffusion\n",
        "\n",
        "**Author:** [Sayak Paul](https://twitter.com/RisingSayak), [Chansung Park](https://twitter.com/algo_diver)<br>\n",
        "**Date created:** 2022/12/28<br>\n",
        "**Last modified:** 2023/01/13<br>\n",
        "**Description:** Fine-tuning Stable Diffusion using a custom image-caption dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ7pWt7atetZ"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This tutorial shows how to fine-tune a\n",
        "[Stable Diffusion model](https://keras.io/guides/keras_cv/generate_images_with_stable_diffusion/)\n",
        "on a custom dataset of `{image, caption}` pairs. We build on top of the fine-tuning\n",
        "script provided by Hugging Face\n",
        "[here](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py).\n",
        "\n",
        "We assume that you have a high-level understanding of the Stable Diffusion model.\n",
        "The following resources can be helpful if you're looking for more information in that regard:\n",
        "\n",
        "* [High-performance image generation using Stable Diffusion in KerasCV](https://keras.io/guides/keras_cv/generate_images_with_stable_diffusion/)\n",
        "* [Stable Diffusion with Diffusers](https://huggingface.co/blog/stable_diffusion)\n",
        "\n",
        "It's highly recommended that you use a GPU with at least 30GB of memory to execute\n",
        "the code.\n",
        "\n",
        "By the end of the guide, you'll be able to generate images of interesting Pokémon:\n",
        "\n",
        "![custom-pokemons](https://i.imgur.com/X4m614M.png)\n",
        "\n",
        "The tutorial relies on KerasCV 0.4.0. Additionally, we need\n",
        "at least TensorFlow 2.11 in order to use AdamW with mixed precision."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade keras-cv"
      ],
      "metadata": {
        "id": "_hwRsRBDw0vy",
        "outputId": "1a3b8765-75a3-404e-a22a-c00ebac074cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-cv in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Collecting keras-cv\n",
            "  Using cached keras_cv-0.9.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-cv) (24.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-cv) (1.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras-cv) (2024.9.11)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from keras-cv) (4.9.7)\n",
            "Requirement already satisfied: keras-core in /usr/local/lib/python3.10/dist-packages (from keras-cv) (0.1.7)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (from keras-cv) (0.3.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras-cv) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras-cv) (4.66.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (3.12.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (0.1.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (8.1.7)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (4.2.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (5.28.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (17.0.0)\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (0.1.6)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (1.13.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (2.5.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (0.10.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (1.16.0)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (0.5.1)\n",
            "Requirement already satisfied: etils>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras-cv) (1.10.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras-cv) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras-cv) (6.4.5)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras-cv) (4.12.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras-cv) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-cv) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-cv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-cv) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-cv) (2024.8.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from promise->tensorflow-datasets->keras-cv) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-core->keras-cv) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-core->keras-cv) (2.18.0)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing->tensorflow-datasets->keras-cv) (0.16)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets->keras-cv) (1.66.0)\n",
            "Collecting protobuf>=3.20 (from tensorflow-datasets->keras-cv)\n",
            "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-core->keras-cv) (0.1.2)\n",
            "Using cached keras_cv-0.9.0-py3-none-any.whl (650 kB)\n",
            "Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, keras-cv\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.28.3\n",
            "    Uninstalling protobuf-5.28.3:\n",
            "      Successfully uninstalled protobuf-5.28.3\n",
            "  Attempting uninstall: keras-cv\n",
            "    Found existing installation: keras-cv 0.4.0\n",
            "    Uninstalling keras-cv-0.4.0:\n",
            "      Successfully uninstalled keras-cv-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.24.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-cv-0.9.0 protobuf-4.25.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1sWJJfSitetc",
        "outputId": "8ece221a-1e01-407e-f97c-ef06ea86af9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-ai-generativelanguage 0.6.10 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-aiplatform 1.71.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.27.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigtable 2.27.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-datastore 2.20.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-firestore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-functions 1.18.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-iam 2.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-language 2.15.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-pubsub 2.27.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-resource-manager 1.13.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-translate 3.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "googleapis-common-protos 1.66.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.19.6 which is incompatible.\n",
            "pandas-gbq 0.24.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.7 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# !pip install keras-cv==0.4.0 -q\n",
        "!pip install --upgrade tensorflow==2.11 -q\n",
        "# !pip install keras-core -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tsrvMCAtetg"
      },
      "source": [
        "## What are we fine-tuning?\n",
        "\n",
        "A Stable Diffusion model can be decomposed into several key models:\n",
        "\n",
        "* A text encoder that projects the input prompt to a latent space. (The caption\n",
        "associated with an image is referred to as the \"prompt\".)\n",
        "* A variational autoencoder (VAE) that projects an input image to a latent space acting\n",
        "as an image vector space.\n",
        "* A diffusion model that refines a latent vector and produces another latent vector, conditioned\n",
        "on the encoded text prompt\n",
        "* A decoder that generates images given a latent vector from the diffusion model.\n",
        "\n",
        "It's worth noting that during the process of generating an image from a text prompt, the\n",
        "image encoder is not typically employed.\n",
        "\n",
        "However, during the process of fine-tuning, the workflow goes like the following:\n",
        "\n",
        "1. An input text prompt is projected to a latent space by the text encoder.\n",
        "2. An input image is projected to a latent space by the image encoder portion of the VAE.\n",
        "3. A small amount of noise is added to the image latent vector for a given timestep.\n",
        "4. The diffusion model uses latent vectors from these two spaces along with a timestep embedding\n",
        "to predict the noise that was added to the image latent.\n",
        "5. A reconstruction loss is calculated between the predicted noise and the original noise\n",
        "added in step 3.\n",
        "6. Finally, the diffusion model parameters are optimized w.r.t this loss using\n",
        "gradient descent.\n",
        "\n",
        "Note that only the diffusion model parameters are updated during fine-tuning, while the\n",
        "(pre-trained) text and the image encoders are kept frozen.\n",
        "\n",
        "Don't worry if this sounds complicated. The code is much simpler than this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhl5_rrQteti"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "I_w1BCOMtetj",
        "outputId": "0dbb642b-0dee-4bf1-9986-a134d753ebf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'keras_cv.src.backend.keras2' has no attribute 'saving'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-177d6ce1dc1e>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstable_diffusion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_tokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstable_diffusion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiffusion_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDiffusionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstable_diffusion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_encoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_cv/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Import everything from /api/ into keras.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m  \u001b[0;31m# Import * ignores names start with \"_\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_cv/api/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbounding_box\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_cv/api/bounding_box/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbounding_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbounding_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_tensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mensure_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbounding_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCENTER_XYWH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_cv/src/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# isort:on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbounding_box\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_cv/src/bounding_box/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbounding_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_decode_deltas_to_boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbounding_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_encode_box_to_deltas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbounding_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_cv/src/bounding_box/converters.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mkeras_cv_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"keras_cv.bounding_box.convert_format\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtf_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m def convert_format(\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m ):\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_cv/src/api_export.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, symbol)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mmaybe_register_serializable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_cv/src/api_export.py\u001b[0m in \u001b[0;36mmaybe_register_serializable\u001b[0;34m(symbol, package)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmaybe_register_serializable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunctionType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"get_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_keras_serializable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpackage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'keras_cv.src.backend.keras2' has no attribute 'saving'"
          ]
        }
      ],
      "source": [
        "from textwrap import wrap\n",
        "import os\n",
        "import json\n",
        "# import keras_cv\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow.experimental.numpy as tnp\n",
        "from keras_cv.models.stable_diffusion.clip_tokenizer import SimpleTokenizer\n",
        "from keras_cv.src.models.stable_diffusion.diffusion_model import DiffusionModel\n",
        "from keras_cv.src.models.stable_diffusion.image_encoder import ImageEncoder\n",
        "from keras_cv.src.models.stable_diffusion.noise_scheduler import NoiseScheduler\n",
        "from keras_cv.src.models.stable_diffusion.text_encoder import TextEncoder\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-Yd29hztetk"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "We use the dataset\n",
        "[Pokémon BLIP captions](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions).\n",
        "However, we'll use a slightly different version which was derived from the original\n",
        "dataset to fit better with `tf.data`. Refer to\n",
        "[the documentation](https://huggingface.co/datasets/sayakpaul/pokemon-blip-original-version)\n",
        "for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOWZYHhotetl"
      },
      "outputs": [],
      "source": [
        "data_path = tf.keras.utils.get_file(\n",
        "    origin=\"https://huggingface.co/datasets/sayakpaul/pokemon-blip-original-version/resolve/main/pokemon_dataset.tar.gz\",\n",
        "    untar=True,\n",
        ")\n",
        "\n",
        "data_frame = pd.read_csv(os.path.join(data_path, \"data.csv\"))\n",
        "\n",
        "data_frame[\"image_path\"] = data_frame[\"image_path\"].apply(\n",
        "    lambda x: os.path.join(data_path, x)\n",
        ")\n",
        "data_frame.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGES_PATH = \"/content/drive/MyDrive/stable_diffusion_finetuning/training_images\"\n",
        "META_DATA = \"/content/drive/MyDrive/stable_diffusion_finetuning/metadata.jsonl\""
      ],
      "metadata": {
        "id": "1CpMSrMpt1iF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKYTYADRtetn"
      },
      "source": [
        "Since we have only 833 `{image, caption}` pairs, we can precompute the text embeddings from\n",
        "the captions. Moreover, the text encoder will be kept frozen during the course of\n",
        "fine-tuning, so we can save some compute by doing this.\n",
        "\n",
        "Before we use the text encoder, we need to tokenize the captions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "\n",
        "# Read the JSONL file\n",
        "with open(META_DATA, \"r\") as file:\n",
        "    for line in file:\n",
        "        # Parse each line as JSON\n",
        "        record = json.loads(line)\n",
        "        # Construct the image path and caption\n",
        "        image_path = os.path.join(IMAGES_PATH, record[\"file_name\"])\n",
        "        caption = record[\"text\"]\n",
        "        # Append the record to the data list\n",
        "        data.append({\"image_path\": image_path, \"caption\": caption})\n",
        "\n",
        "# Create a DataFrame\n",
        "data_frame = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(data_frame.head())\n"
      ],
      "metadata": {
        "id": "J-XklUfRxo3L",
        "outputId": "74cb3c5c-eda7-4e52-ff8b-5613b055b3eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          image_path  \\\n",
            "0  /content/drive/MyDrive/stable_diffusion_finetu...   \n",
            "1  /content/drive/MyDrive/stable_diffusion_finetu...   \n",
            "2  /content/drive/MyDrive/stable_diffusion_finetu...   \n",
            "3  /content/drive/MyDrive/stable_diffusion_finetu...   \n",
            "4  /content/drive/MyDrive/stable_diffusion_finetu...   \n",
            "\n",
            "                                             caption  \n",
            "0       anime character, transparent and transparent  \n",
            "1                       two people with a man's face  \n",
            "2  the avatar characters with two men, one in fro...  \n",
            "3    the south park character from south and america  \n",
            "4                              a toy story character  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_62mxhRJteto"
      },
      "outputs": [],
      "source": [
        "# The padding token and maximum prompt length are specific to the text encoder.\n",
        "# If you're using a different text encoder be sure to change them accordingly.\n",
        "PADDING_TOKEN = 49407\n",
        "MAX_PROMPT_LENGTH = 77\n",
        "\n",
        "# Load the tokenizer.\n",
        "tokenizer = SimpleTokenizer()\n",
        "\n",
        "#  Method to tokenize and pad the tokens.\n",
        "def process_text(caption):\n",
        "    tokens = tokenizer.encode(caption)\n",
        "    tokens = tokens + [PADDING_TOKEN] * (MAX_PROMPT_LENGTH - len(tokens))\n",
        "    return np.array(tokens)\n",
        "\n",
        "\n",
        "# Collate the tokenized captions into an array.\n",
        "tokenized_texts = np.empty((len(data_frame), MAX_PROMPT_LENGTH))\n",
        "\n",
        "all_captions = list(data_frame[\"caption\"].values)\n",
        "for i, caption in enumerate(all_captions):\n",
        "    tokenized_texts[i] = process_text(caption)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAfesh0ztetq"
      },
      "source": [
        "## Prepare a `tf.data.Dataset`\n",
        "\n",
        "In this section, we'll prepare a `tf.data.Dataset` object from the input image file paths\n",
        "and their corresponding caption tokens. The section will include the following:\n",
        "\n",
        "* Pre-computation of the text embeddings from the tokenized captions.\n",
        "* Loading and augmentation of the input images.\n",
        "* Shuffling and batching of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1fyStFGptetr",
        "outputId": "0368a6a2-f86d-4c11-c7ba-78756ac97db5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 902
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:1381: UserWarning: Layer 'clip_encoder_layer' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
            "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
            "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
            "Exception encountered: ''Exception encountered when calling CLIPAttention.call().\n",
            "\n",
            "\u001b[1mpred must not be a Python bool\u001b[0m\n",
            "\n",
            "Arguments received by CLIPAttention.call():\n",
            "  • inputs=tf.Tensor(shape=(None, 77, 768), dtype=float32)\n",
            "  • attention_mask=None''\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'clip_encoder_layer', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Exception encountered when calling CLIPEncoderLayer.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'clip_encoder_layer' (of type CLIPEncoderLayer). Either the `CLIPEncoderLayer.call()` method is incorrect, or you need to implement the `CLIPEncoderLayer.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nException encountered when calling CLIPAttention.call().\n\n\u001b[1mpred must not be a Python bool\u001b[0m\n\nArguments received by CLIPAttention.call():\n  • inputs=tf.Tensor(shape=(None, 77, 768), dtype=float32)\n  • attention_mask=None\u001b[0m\n\nArguments received by CLIPEncoderLayer.call():\n  • args=('<KerasTensor shape=(None, 77, 768), dtype=float32, sparse=False, name=keras_tensor_1>',)\n  • kwargs=<class 'inspect._empty'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-9dadda448ced>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     ]\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtext_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_PROMPT_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_cv/src/models/stable_diffusion/text_encoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, max_length, vocab_size, name, download_weights)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCLIPEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCLIPEncoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquick_gelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_cv/src/models/stable_diffusion/text_encoder.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_cv/src/models/stable_diffusion/text_encoder.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, attention_mask)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcausal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             attention_mask = ops.triu(\n\u001b[0m\u001b[1;32m    137\u001b[0m                 \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling CLIPEncoderLayer.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'clip_encoder_layer' (of type CLIPEncoderLayer). Either the `CLIPEncoderLayer.call()` method is incorrect, or you need to implement the `CLIPEncoderLayer.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nException encountered when calling CLIPAttention.call().\n\n\u001b[1mpred must not be a Python bool\u001b[0m\n\nArguments received by CLIPAttention.call():\n  • inputs=tf.Tensor(shape=(None, 77, 768), dtype=float32)\n  • attention_mask=None\u001b[0m\n\nArguments received by CLIPEncoderLayer.call():\n  • args=('<KerasTensor shape=(None, 77, 768), dtype=float32, sparse=False, name=keras_tensor_1>',)\n  • kwargs=<class 'inspect._empty'>"
          ]
        }
      ],
      "source": [
        "RESOLUTION = 256\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "POS_IDS = tf.convert_to_tensor([list(range(MAX_PROMPT_LENGTH))], dtype=tf.int32)\n",
        "\n",
        "augmenter = keras.Sequential(\n",
        "    layers=[\n",
        "        keras_cv.src.layers.CenterCrop(RESOLUTION, RESOLUTION),\n",
        "        keras_cv.layers.RandomFlip(),\n",
        "        tf.keras.layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
        "    ]\n",
        ")\n",
        "text_encoder = TextEncoder(MAX_PROMPT_LENGTH)\n",
        "\n",
        "\n",
        "def process_image(image_path, tokenized_text):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.io.decode_png(image, 3)\n",
        "    image = tf.image.resize(image, (RESOLUTION, RESOLUTION))\n",
        "    return image, tokenized_text\n",
        "\n",
        "\n",
        "def apply_augmentation(image_batch, token_batch):\n",
        "    return augmenter(image_batch), token_batch\n",
        "\n",
        "\n",
        "def run_text_encoder(image_batch, token_batch):\n",
        "    return (\n",
        "        image_batch,\n",
        "        token_batch,\n",
        "        text_encoder([token_batch, POS_IDS], training=False),\n",
        "    )\n",
        "\n",
        "\n",
        "def prepare_dict(image_batch, token_batch, encoded_text_batch):\n",
        "    return {\n",
        "        \"images\": image_batch,\n",
        "        \"tokens\": token_batch,\n",
        "        \"encoded_text\": encoded_text_batch,\n",
        "    }\n",
        "\n",
        "\n",
        "def prepare_dataset(image_paths, tokenized_texts, batch_size=1):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, tokenized_texts))\n",
        "    dataset = dataset.shuffle(batch_size * 10)\n",
        "    dataset = dataset.map(process_image, num_parallel_calls=AUTO).batch(batch_size)\n",
        "    dataset = dataset.map(apply_augmentation, num_parallel_calls=AUTO)\n",
        "    dataset = dataset.map(run_text_encoder, num_parallel_calls=AUTO)\n",
        "    dataset = dataset.map(prepare_dict, num_parallel_calls=AUTO)\n",
        "    return dataset.prefetch(AUTO)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8q_cziftets"
      },
      "source": [
        "The baseline Stable Diffusion model was trained using images with 512x512 resolution. It's\n",
        "unlikely for a model that's trained using higher-resolution images to transfer well to\n",
        "lower-resolution images. However, the current model will lead to OOM if we keep the\n",
        "resolution to 512x512 (without enabling mixed-precision). Therefore, in the interest of\n",
        "interactive demonstrations, we kept the input resolution to 256x256."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgFB07Zptets"
      },
      "outputs": [],
      "source": [
        "# Prepare the dataset.\n",
        "training_dataset = prepare_dataset(\n",
        "    np.array(data_frame[\"image_path\"]), tokenized_texts, batch_size=4\n",
        ")\n",
        "\n",
        "# Take a sample batch and investigate.\n",
        "sample_batch = next(iter(training_dataset))\n",
        "\n",
        "for k in sample_batch:\n",
        "    print(k, sample_batch[k].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uToU0cZOtets"
      },
      "source": [
        "We can also take a look at the training images and their corresponding captions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IBvFFfXtett"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "for i in range(3):\n",
        "    ax = plt.subplot(1, 4, i + 1)\n",
        "    plt.imshow((sample_batch[\"images\"][i] + 1) / 2)\n",
        "\n",
        "    text = tokenizer.decode(sample_batch[\"tokens\"][i].numpy().squeeze())\n",
        "    text = text.replace(\"<|startoftext|>\", \"\")\n",
        "    text = text.replace(\"<|endoftext|>\", \"\")\n",
        "    text = \"\\n\".join(wrap(text, 12))\n",
        "    plt.title(text, fontsize=15)\n",
        "\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpoJDQ1ntett"
      },
      "source": [
        "## A trainer class for the fine-tuning loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c7Y8wPwtett"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Trainer(tf.keras.Model):\n",
        "    # Reference:\n",
        "    # https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        diffusion_model,\n",
        "        vae,\n",
        "        noise_scheduler,\n",
        "        use_mixed_precision=False,\n",
        "        max_grad_norm=1.0,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.diffusion_model = diffusion_model\n",
        "        self.vae = vae\n",
        "        self.noise_scheduler = noise_scheduler\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "\n",
        "        self.use_mixed_precision = use_mixed_precision\n",
        "        self.vae.trainable = False\n",
        "\n",
        "    def train_step(self, inputs):\n",
        "        images = inputs[\"images\"]\n",
        "        encoded_text = inputs[\"encoded_text\"]\n",
        "        batch_size = tf.shape(images)[0]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Project image into the latent space and sample from it.\n",
        "            latents = self.sample_from_encoder_outputs(self.vae(images, training=False))\n",
        "            # Know more about the magic number here:\n",
        "            # https://keras.io/examples/generative/fine_tune_via_textual_inversion/\n",
        "            latents = latents * 0.18215\n",
        "\n",
        "            # Sample noise that we'll add to the latents.\n",
        "            noise = tf.random.normal(tf.shape(latents))\n",
        "\n",
        "            # Sample a random timestep for each image.\n",
        "            timesteps = tnp.random.randint(\n",
        "                0, self.noise_scheduler.train_timesteps, (batch_size,)\n",
        "            )\n",
        "\n",
        "            # Add noise to the latents according to the noise magnitude at each timestep\n",
        "            # (this is the forward diffusion process).\n",
        "            noisy_latents = self.noise_scheduler.add_noise(\n",
        "                tf.cast(latents, noise.dtype), noise, timesteps\n",
        "            )\n",
        "\n",
        "            # Get the target for loss depending on the prediction type\n",
        "            # just the sampled noise for now.\n",
        "            target = noise  # noise_schedule.predict_epsilon == True\n",
        "\n",
        "            # Predict the noise residual and compute loss.\n",
        "            timestep_embedding = tf.map_fn(\n",
        "                lambda t: self.get_timestep_embedding(t), timesteps, dtype=tf.float32\n",
        "            )\n",
        "            timestep_embedding = tf.squeeze(timestep_embedding, 1)\n",
        "            model_pred = self.diffusion_model(\n",
        "                [noisy_latents, timestep_embedding, encoded_text], training=True\n",
        "            )\n",
        "            loss = self.compiled_loss(target, model_pred)\n",
        "            if self.use_mixed_precision:\n",
        "                loss = self.optimizer.get_scaled_loss(loss)\n",
        "\n",
        "        # Update parameters of the diffusion model.\n",
        "        trainable_vars = self.diffusion_model.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        if self.use_mixed_precision:\n",
        "            gradients = self.optimizer.get_unscaled_gradients(gradients)\n",
        "        gradients = [tf.clip_by_norm(g, self.max_grad_norm) for g in gradients]\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def get_timestep_embedding(self, timestep, dim=320, max_period=10000):\n",
        "        half = dim // 2\n",
        "        log_max_period = tf.math.log(tf.cast(max_period, tf.float32))\n",
        "        freqs = tf.math.exp(\n",
        "            -log_max_period * tf.range(0, half, dtype=tf.float32) / half\n",
        "        )\n",
        "        args = tf.convert_to_tensor([timestep], dtype=tf.float32) * freqs\n",
        "        embedding = tf.concat([tf.math.cos(args), tf.math.sin(args)], 0)\n",
        "        embedding = tf.reshape(embedding, [1, -1])\n",
        "        return embedding\n",
        "\n",
        "    def sample_from_encoder_outputs(self, outputs):\n",
        "        mean, logvar = tf.split(outputs, 2, axis=-1)\n",
        "        logvar = tf.clip_by_value(logvar, -30.0, 20.0)\n",
        "        std = tf.exp(0.5 * logvar)\n",
        "        sample = tf.random.normal(tf.shape(mean), dtype=mean.dtype)\n",
        "        return mean + std * sample\n",
        "\n",
        "    def save_weights(self, filepath, overwrite=True, save_format=None, options=None):\n",
        "        # Overriding this method will allow us to use the `ModelCheckpoint`\n",
        "        # callback directly with this trainer class. In this case, it will\n",
        "        # only checkpoint the `diffusion_model` since that's what we're training\n",
        "        # during fine-tuning.\n",
        "        self.diffusion_model.save_weights(\n",
        "            filepath=filepath,\n",
        "            overwrite=overwrite,\n",
        "            save_format=save_format,\n",
        "            options=options,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkWdKyNatetu"
      },
      "source": [
        "One important implementation detail to note here: Instead of directly taking\n",
        "the latent vector produced by the image encoder (which is a VAE), we sample from the\n",
        "mean and log-variance predicted by it. This way, we can achieve better sample\n",
        "quality and diversity.\n",
        "\n",
        "It's common to add support for mixed-precision training along with exponential\n",
        "moving averaging of model weights for fine-tuning these models. However, in the interest\n",
        "of brevity, we discard those elements. More on this later in the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daZvDZjetetv"
      },
      "source": [
        "## Initialize the trainer and compile it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk0H433btetv"
      },
      "outputs": [],
      "source": [
        "# Enable mixed-precision training if the underlying GPU has tensor cores.\n",
        "USE_MP = True\n",
        "if USE_MP:\n",
        "    keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "\n",
        "image_encoder = ImageEncoder()\n",
        "diffusion_ft_trainer = Trainer(\n",
        "    diffusion_model=DiffusionModel(RESOLUTION, RESOLUTION, MAX_PROMPT_LENGTH),\n",
        "    # Remove the top layer from the encoder, which cuts off the variance and only\n",
        "    # returns the mean.\n",
        "    vae=tf.keras.Model(\n",
        "        image_encoder.input,\n",
        "        image_encoder.layers[-2].output,\n",
        "    ),\n",
        "    noise_scheduler=NoiseScheduler(),\n",
        "    use_mixed_precision=USE_MP,\n",
        ")\n",
        "\n",
        "# These hyperparameters come from this tutorial by Hugging Face:\n",
        "# https://huggingface.co/docs/diffusers/training/text2image\n",
        "lr = 1e-5\n",
        "beta_1, beta_2 = 0.9, 0.999\n",
        "weight_decay = (1e-2,)\n",
        "epsilon = 1e-08\n",
        "\n",
        "optimizer = tf.keras.optimizers.experimental.AdamW(\n",
        "    learning_rate=lr,\n",
        "    weight_decay=weight_decay,\n",
        "    beta_1=beta_1,\n",
        "    beta_2=beta_2,\n",
        "    epsilon=epsilon,\n",
        ")\n",
        "diffusion_ft_trainer.compile(optimizer=optimizer, loss=\"mse\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmA6CZzDtetw"
      },
      "source": [
        "## Fine-tuning\n",
        "\n",
        "To keep the runtime of this tutorial short, we just fine-tune for an epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRrIPXvLtetw"
      },
      "outputs": [],
      "source": [
        "epochs = 1\n",
        "ckpt_path = \"finetuned_stable_diffusion.h5\"\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    ckpt_path,\n",
        "    save_weights_only=True,\n",
        "    monitor=\"loss\",\n",
        "    mode=\"min\",\n",
        ")\n",
        "diffusion_ft_trainer.fit(training_dataset, epochs=epochs, callbacks=[ckpt_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng-HP207tetx"
      },
      "source": [
        "## Inference\n",
        "\n",
        "We fine-tuned the model for 60 epochs on an image resolution of 512x512. To allow\n",
        "training with this resolution, we incorporated mixed-precision support. You can\n",
        "check out\n",
        "[this repository](https://github.com/sayakpaul/stabe-diffusion-keras-ft)\n",
        "for more details. It additionally provides support for exponential moving averaging of\n",
        "the fine-tuned model parameters and model checkpointing.\n",
        "\n",
        "\n",
        "For this section, we'll use the checkpoint derived after 60 epochs of fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zg4j2P0Ttetx"
      },
      "outputs": [],
      "source": [
        "weights_path = tf.keras.utils.get_file(\n",
        "    origin=\"https://huggingface.co/sayakpaul/kerascv_sd_pokemon_finetuned/resolve/main/ckpt_epochs_72_res_512_mp_True.h5\"\n",
        ")\n",
        "\n",
        "img_height = img_width = 512\n",
        "pokemon_model = keras_cv.models.StableDiffusion(\n",
        "    img_width=img_width, img_height=img_height\n",
        ")\n",
        "# We just reload the weights of the fine-tuned diffusion model.\n",
        "pokemon_model.diffusion_model.load_weights(weights_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1S2Lh4-tety"
      },
      "source": [
        "Now, we can take this model for a test-drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hRBRYWVtety"
      },
      "outputs": [],
      "source": [
        "prompts = [\"Yoda\", \"Hello Kitty\", \"A pokemon with red eyes\"]\n",
        "images_to_generate = 3\n",
        "outputs = {}\n",
        "\n",
        "for prompt in prompts:\n",
        "    generated_images = pokemon_model.text_to_image(\n",
        "        prompt, batch_size=images_to_generate, unconditional_guidance_scale=40\n",
        "    )\n",
        "    outputs.update({prompt: generated_images})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhfCydKstety"
      },
      "source": [
        "With 60 epochs of fine-tuning (a good number is about 70), the generated images were not\n",
        "up to the mark. So, we experimented with the number of steps Stable Diffusion takes\n",
        "during the inference time and the `unconditional_guidance_scale` parameter.\n",
        "\n",
        "We found the best results with this checkpoint with `unconditional_guidance_scale` set to\n",
        "40."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fyVvYwatetz"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_images(images, title):\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    for i in range(len(images)):\n",
        "        ax = plt.subplot(1, len(images), i + 1)\n",
        "        plt.imshow(images[i])\n",
        "        plt.title(title, fontsize=12)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "\n",
        "for prompt in outputs:\n",
        "    plot_images(outputs[prompt], prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naBIvLK8tetz"
      },
      "source": [
        "We can notice that the model has started adapting to the style of our dataset. You can\n",
        "check the\n",
        "[accompanying repository](https://github.com/sayakpaul/stable-diffusion-keras-ft#results)\n",
        "for more comparisons and commentary. If you're feeling adventurous to try out a demo,\n",
        "you can check out\n",
        "[this resource](https://huggingface.co/spaces/sayakpaul/pokemon-sd-kerascv)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w1NYQ6htetz"
      },
      "source": [
        "## Conclusion and acknowledgements\n",
        "\n",
        "We demonstrated how to fine-tune the Stable Diffusion model on a custom dataset. While\n",
        "the results are far from aesthetically pleasing, we believe with more epochs of\n",
        "fine-tuning, they will likely improve. To enable that, having support for gradient\n",
        "accumulation and distributed training is crucial. This can be thought of as the next step\n",
        "in this tutorial.\n",
        "\n",
        "There is another interesting way in which Stable Diffusion models can be fine-tuned,\n",
        "called textual inversion. You can refer to\n",
        "[this tutorial](https://keras.io/examples/generative/fine_tune_via_textual_inversion/)\n",
        "to know more about it.\n",
        "\n",
        "We'd like to acknowledge the GCP Credit support from ML Developer Programs' team at\n",
        "Google. We'd like to thank the Hugging Face team for providing the\n",
        "[fine-tuning script](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py)\n",
        ". It's very readable and easy to understand."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "finetune_stable_diffusion",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}